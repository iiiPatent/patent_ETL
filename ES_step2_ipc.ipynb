{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step1  不要點\n",
    "# import os, sys\n",
    "# fff = open('E:/patent_data/catalog/.txt','w')\n",
    "# path = \"E:/patent_data/catalog/cat\" #所有擋案的路徑\n",
    "\n",
    "# list_40001 = os.listdir(path)\n",
    "# for number40001 in list_40001:\n",
    "#     xml092134555_list = os.listdir(path +'/'+ number40001 )\n",
    "#     for xml092134555 in xml092134555_list:\n",
    "#         final_path = path + '/' + number40001 + '/' + xml092134555\n",
    "#         final = final_path + '\\n'\n",
    "#         fff.write(final)\n",
    "# fff.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step2\n",
    "from bs4 import BeautifulSoup as bs\n",
    "def buildIPC(single_path):\n",
    "    dic = {'this_id':'',\n",
    "        'classification-ipc-main-classification':'',\n",
    "      'classification-ipc-further-classification':'',\n",
    "      'classification-locarno-main-classification':'',\n",
    "      'classification-locarno-further-classification':''}\n",
    "    \n",
    "    f = open(single_path,'r')\n",
    "    soup = bs(f.read())\n",
    "    dic['this_id'] = soup.select('application-reference')[0].select('doc-number')[0].text.encode('utf-8')\n",
    "    #print this_id\n",
    "\n",
    "    if soup.select('classification-ipc'):\n",
    "        #print 'ipc:-----------------------------'\n",
    "        #print soup.select('classification-ipc')[0].select('main-classification')[0].text.encode('utf-8')\n",
    "        dic['classification-ipc-main-classification'] = soup.select('classification-ipc')[0].select('main-classification')[0].text.encode('utf-8')\n",
    "        if soup.select('classification-ipc')[0].select('further-classification'):\n",
    "            #print soup.select('classification-ipc')[0].select('further-classification')[0].text.encode('utf-8')\n",
    "            dic['classification-ipc-further-classification'] = soup.select('classification-ipc')[0].select('further-classification')[0].text.encode('utf-8')\n",
    "    elif soup.select('classification-locarno'):\n",
    "        #print 'loc:---------------------------'\n",
    "        #print soup.select('classification-locarno')[0].select('main-classification')[0].text.encode('utf-8')\n",
    "        dic['classification-locarno-main-classification'] = soup.select('classification-locarno')[0].select('main-classification')[0].text.encode('utf-8')\n",
    "        if soup.select('classification-locarno')[0].select('further-classification'):\n",
    "            #print soup.select('classification-locarno')[0].select('further-classification')[0].text.encode('utf-8')\n",
    "            dic['classification-locarno-further-classification'] = soup.select('classification-locarno')[0].select('further-classification')[0].text.encode('utf-8')\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import elasticsearch\n",
    "es = Elasticsearch('10.120.26.16:9200')\n",
    "\n",
    "#doc = {\"hi\": \"yes\"}\n",
    "ipc = '10-155'\n",
    "dic = {\"loc\":\"1589\",\"bre\":\"\",\"qewty\":\"483\",\"wr\":\"\"}\n",
    "\n",
    "\n",
    "#update內文有兩種選擇scrip, doc\n",
    "#這裡是用doc的語法, {\"doc\":{\"欄位\":\"值\",.....}}, 但是需要把它變成str\n",
    "for key in dic:\n",
    "    doc = '''{\"doc\":{%s:%s}}''' %(key,dic[key])\n",
    "    print type(doc)\n",
    "    print doc\n",
    "    try:\n",
    "        es.update(index=\"test-index\",doc_type=\"tweet\",id=1,body=doc, ignore=500)\n",
    "    except elasticsearch.exceptions.NotFoundError :\n",
    "        print 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BigData\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "WARNING:elasticsearch:POST http://10.120.26.16:9200/henry/ray/095109063/_update [status:N/A request:0.000s]\n",
      "Traceback (most recent call last):\n",
      "  File \"build\\bdist.win-amd64\\egg\\elasticsearch\\connection\\http_urllib3.py\", line 78, in perform_request\n",
      "    response = self.pool.urlopen(method, url, body, retries=False, headers=self.headers, **kw)\n",
      "  File \"C:\\Users\\BigData\\Anaconda2\\lib\\site-packages\\urllib3\\connectionpool.py\", line 609, in urlopen\n",
      "    _stacktrace=sys.exc_info()[2])\n",
      "  File \"C:\\Users\\BigData\\Anaconda2\\lib\\site-packages\\urllib3\\util\\retry.py\", line 222, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"C:\\Users\\BigData\\Anaconda2\\lib\\site-packages\\urllib3\\connectionpool.py\", line 559, in urlopen\n",
      "    body=body, headers=headers)\n",
      "  File \"C:\\Users\\BigData\\Anaconda2\\lib\\site-packages\\urllib3\\connectionpool.py\", line 353, in _make_request\n",
      "    conn.request(method, url, **httplib_request_kw)\n",
      "  File \"C:\\Users\\BigData\\Anaconda2\\lib\\httplib.py\", line 1053, in request\n",
      "    self._send_request(method, url, body, headers)\n",
      "  File \"C:\\Users\\BigData\\Anaconda2\\lib\\httplib.py\", line 1087, in _send_request\n",
      "    self.putrequest(method, url, **skips)\n",
      "  File \"C:\\Users\\BigData\\Anaconda2\\lib\\httplib.py\", line 935, in putrequest\n",
      "    raise CannotSendRequest()\n",
      "ProtocolError: ('Connection aborted.', CannotSendRequest())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:E:/patent_data/catalog/cat40/40011/095108693.xml\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-aa5a051cdda6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0msingle_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuildIPC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msingle_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-f05014c13574>\u001b[0m in \u001b[0;36mbuildIPC\u001b[1;34m(single_path)\u001b[0m\n\u001b[0;32m      8\u001b[0m       'classification-locarno-further-classification':''}\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msingle_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'this_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'application-reference'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'doc-number'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#step3    #要改一處\n",
    "from elasticsearch import Elasticsearch\n",
    "import elasticsearch\n",
    "es = Elasticsearch('10.120.26.16:9200')\n",
    "\n",
    "ff = open('e:/step2_ipc.txt','w')\n",
    "count = 0\n",
    "\n",
    "f = open('E:\\\\patent_data\\\\catalog\\\\complete_path_catelog\\\\final_catelog.txt', 'r')  ###\n",
    "for line in f.readlines():\n",
    "    \n",
    "    single_path = line.split()[0]\n",
    "    dic = buildIPC(single_path)\n",
    "    \n",
    "    try:\n",
    "        for key in dic:\n",
    "            doc = '''{\"doc\":{\"%s\":\"%s\"}}''' %(key,dic[key])\n",
    "            es.update(index=\"henry\",doc_type=\"ray\",id=dic[\"this_id\"],body=doc, ignore=400)\n",
    "            count +=1\n",
    "    #連線時間太短\n",
    "    except elasticsearch.exceptions.ConnectionError:\n",
    "        print 'timeout:' + single_path\n",
    "        ff.write('timeout:' + single_path + '\\n')\n",
    "        pass\n",
    "    #找不到對應的專利本文\n",
    "    except elasticsearch.exceptions.NotFoundError :\n",
    "        print 'NotFoundError:' + single_path\n",
    "        ff.write('NotFoundError:' + single_path + '\\n')\n",
    "        pass\n",
    "    #其他錯誤\n",
    "    except:\n",
    "        print 'error:' + single_path\n",
    "        ff.write('error:' + single_path + '\\n')\n",
    "        pass\n",
    "ff.write('=================================================\\n')\n",
    "ff.write('total:'+str(count)+'\\n')\n",
    "\n",
    "print 'total: ' + str(count)\n",
    "f.close()\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "亂碼碼碼碼碼!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "E:/patent_data/catalog/cat41/41018/099142874.xml\n",
    "E:/patent_data/catalog/cat41/41018/100116911.xml\n",
    "E:/patent_data/catalog/cat41/41018/100136373.xml\n",
    "E:/patent_data/catalog/cat41/41022/102305968.xml\n",
    "E:/patent_data/catalog/cat41/41022/103202318.xml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
